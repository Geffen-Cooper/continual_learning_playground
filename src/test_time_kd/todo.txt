Q1. test cosine similarity between two untrained domains in successsion
    -a:does fine-tuning on a domain enable distinction between it and other domains
    -b:how much fine-tuning is required
    -c:do we finetune by minimizing cosine similarity or something else
    -d:how does this apply to other datasets
    -e:what if the student model did not start from the teacher initialization (e.g. we use v3_small as student)

A1. found a bug so the method doesn't actually work

============================================================================

Q2. How can we improve the student
    -try training offline in batches
    -try the 128x128 v3 model

A2. It works better but improvement is negligible. For example, the student went
    from ~34% to ~36% after distillation on a couple hundred images. At least it 
    is learning with relatively few images, but the improvement is small. We want
    at least 5% if we are using a lot of MACs. Also, we need to look at the
    avergae boost by plotting on the acc vs MACs diagram
    -same for the 128x128, improvement is negligible

============================================================================

Let's try to detect domain shift using the entropy of the softmax output
like other people do. We can also show the confidence diagrams for the extra
corruptions as justification without overfitting.
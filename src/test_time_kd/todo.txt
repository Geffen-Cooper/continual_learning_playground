Q1. test cosine similarity between two untrained domains in successsion
    -a:does fine-tuning on a domain enable distinction between it and other domains
    -b:how much fine-tuning is required
    -c:do we finetune by minimizing cosine similarity or something else
    -d:how does this apply to other datasets
    -e:what if the student model did not start from the teacher initialization (e.g. we use v3_small as student)

A1. found a bug so the method doesn't actually work

============================================================================

Q2. How can we improve the student
    -try training offline in batches
    -try the 128x128 v3 model

A2. It works better but improvement is negligible. For example, the student went
    from ~34% to ~36% after distillation on a couple hundred images. At least it 
    is learning with relatively few images, but the improvement is small. We want
    at least 5% if we are using a lot of MACs. Also, we need to look at the
    avergae boost by plotting on the acc vs MACs diagram
    -same for the 128x128, improvement is negligible

Update, batched learning seems to be necessary as online updates are too noisy,
as a result I use gradient accumulation. If we can find a novel way to update in
the right direction very fast (i.e. meta learning) then that would be ideal

============================================================================

Let's try to detect domain shift using the entropy of the softmax output
like other people do. We can also show the confidence diagrams for the extra
corruptions as justification without overfitting.

entropy doesn't work


============================================================================

for model updates, maybe supress small gradients and amplify strong gradients,
see how that effects the accuracy

does resetting the weights for each domain shift help? i.e. is it easier to adapt from
the original model or the current model
    NO, resetting the model doesn't help

=====================================
To do
-plot the naive methods for a sampling of thresholds, 10%, 20%, 30%, 40%, 50% as baselines
-from these baselines, see if we can improve through selective training?
    -i.e. if we train, can we make it so that we are below the threshold less? Check using model efficiency
-change from MACS to flops
-see if we can possibly plot the MSDNET or RANNET or ensemble of resenet on here along with there ideal curves



=================================================

=================================================

=================================================

Analyzing domain shift

Q1. How is top 5 or top 10 score affected after domain shift? Is it more robust than
    top 1? This would suggest that the shift was small enough so that the model 
    still assigns some non-negligible confidence to the class. Another thing to
    check is how far the shifted points are from the source centroids relative to
    all the other classes.
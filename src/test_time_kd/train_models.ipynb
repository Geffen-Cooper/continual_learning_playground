{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import *\n",
    "from train import *\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=576, out_features=1280, bias=True)\n",
      "  (1): Hardswish()\n",
      "  (2): Dropout(p=0.2, inplace=True)\n",
      "  (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
      ")\n",
      "Train Epoch: 0 [0/1153050 (0%)] train loss: 3.761, lr: 0.00400000\n",
      "Train Epoch: 0 [51200/1153050 (4%)] train loss: 1.661, lr: 0.00396000\n",
      "Train Epoch: 0 [102400/1153050 (9%)] train loss: 2.307, lr: 0.00392040\n",
      "Train Epoch: 0 [153600/1153050 (13%)] train loss: 2.407, lr: 0.00388120\n",
      "Train Epoch: 0 [204800/1153050 (18%)] train loss: 1.932, lr: 0.00384238\n",
      "Train Epoch: 0 [256000/1153050 (22%)] train loss: 2.057, lr: 0.00380396\n",
      "Train Epoch: 0 [307200/1153050 (27%)] train loss: 1.654, lr: 0.00376592\n",
      "Train Epoch: 0 [358400/1153050 (31%)] train loss: 2.204, lr: 0.00372826\n",
      "Train Epoch: 0 [409600/1153050 (36%)] train loss: 1.713, lr: 0.00369098\n",
      "Train Epoch: 0 [460800/1153050 (40%)] train loss: 1.780, lr: 0.00365407\n",
      "Train Epoch: 0 [512000/1153050 (44%)] train loss: 1.801, lr: 0.00361753\n",
      "Train Epoch: 0 [563200/1153050 (49%)] train loss: 1.679, lr: 0.00358135\n",
      "Train Epoch: 0 [614400/1153050 (53%)] train loss: 1.540, lr: 0.00354554\n",
      "Train Epoch: 0 [665600/1153050 (58%)] train loss: 1.504, lr: 0.00351008\n",
      "Train Epoch: 0 [716800/1153050 (62%)] train loss: 1.506, lr: 0.00347498\n",
      "Train Epoch: 0 [768000/1153050 (67%)] train loss: 1.428, lr: 0.00344023\n",
      "Train Epoch: 0 [819200/1153050 (71%)] train loss: 1.395, lr: 0.00340583\n",
      "Train Epoch: 0 [870400/1153050 (75%)] train loss: 1.661, lr: 0.00337177\n",
      "Train Epoch: 0 [921600/1153050 (80%)] train loss: 1.710, lr: 0.00333806\n",
      "Train Epoch: 0 [972800/1153050 (84%)] train loss: 1.273, lr: 0.00330467\n",
      "Train Epoch: 0 [1024000/1153050 (89%)] train loss: 1.765, lr: 0.00327163\n",
      "Train Epoch: 0 [1075200/1153050 (93%)] train loss: 1.806, lr: 0.00323891\n",
      "Train Epoch: 0 [1126400/1153050 (98%)] train loss: 2.073, lr: 0.00320652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:31<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [468416/1153050 (100%)] train loss: 2.143, val loss: 1.887, val acc: 0.555, top5: 0.791, lr: 0.00317446\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 0, val accuracy: 0.5552502790418133\n",
      "Train Epoch: 1 [0/1153050 (0%)] train loss: 2.061, lr: 0.00317446\n",
      "Train Epoch: 1 [51200/1153050 (4%)] train loss: 1.490, lr: 0.00157136\n",
      "Train Epoch: 1 [102400/1153050 (9%)] train loss: 1.473, lr: 0.00155564\n",
      "Train Epoch: 1 [153600/1153050 (13%)] train loss: 1.279, lr: 0.00154009\n",
      "Train Epoch: 1 [204800/1153050 (18%)] train loss: 1.567, lr: 0.00152469\n",
      "Train Epoch: 1 [256000/1153050 (22%)] train loss: 1.320, lr: 0.00150944\n",
      "Train Epoch: 1 [307200/1153050 (27%)] train loss: 1.413, lr: 0.00149434\n",
      "Train Epoch: 1 [358400/1153050 (31%)] train loss: 1.461, lr: 0.00147940\n",
      "Train Epoch: 1 [409600/1153050 (36%)] train loss: 2.039, lr: 0.00146461\n",
      "Train Epoch: 1 [460800/1153050 (40%)] train loss: 1.464, lr: 0.00144996\n",
      "Train Epoch: 1 [512000/1153050 (44%)] train loss: 1.625, lr: 0.00143546\n",
      "Train Epoch: 1 [563200/1153050 (49%)] train loss: 1.530, lr: 0.00142111\n",
      "Train Epoch: 1 [614400/1153050 (53%)] train loss: 1.513, lr: 0.00140690\n",
      "Train Epoch: 1 [665600/1153050 (58%)] train loss: 1.372, lr: 0.00139283\n",
      "Train Epoch: 1 [716800/1153050 (62%)] train loss: 1.516, lr: 0.00137890\n",
      "Train Epoch: 1 [768000/1153050 (67%)] train loss: 1.171, lr: 0.00136511\n",
      "Train Epoch: 1 [819200/1153050 (71%)] train loss: 1.314, lr: 0.00135146\n",
      "Train Epoch: 1 [870400/1153050 (75%)] train loss: 1.160, lr: 0.00133794\n",
      "Train Epoch: 1 [921600/1153050 (80%)] train loss: 0.976, lr: 0.00132456\n",
      "Train Epoch: 1 [972800/1153050 (84%)] train loss: 1.219, lr: 0.00131132\n",
      "Train Epoch: 1 [1024000/1153050 (89%)] train loss: 1.271, lr: 0.00129821\n",
      "Train Epoch: 1 [1075200/1153050 (93%)] train loss: 1.701, lr: 0.00128522\n",
      "Train Epoch: 1 [1126400/1153050 (98%)] train loss: 1.504, lr: 0.00127237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:30<00:00, 13.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [468416/1153050 (100%)] train loss: 1.447, val loss: 1.479, val acc: 0.633, top5: 0.853, lr: 0.00125965\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 1, val accuracy: 0.63317124191169\n",
      "Train Epoch: 2 [0/1153050 (0%)] train loss: 1.316, lr: 0.00125965\n",
      "Train Epoch: 2 [51200/1153050 (4%)] train loss: 1.186, lr: 0.00062353\n",
      "Train Epoch: 2 [102400/1153050 (9%)] train loss: 1.017, lr: 0.00061729\n",
      "Train Epoch: 2 [153600/1153050 (13%)] train loss: 0.915, lr: 0.00061112\n",
      "Train Epoch: 2 [204800/1153050 (18%)] train loss: 1.367, lr: 0.00060501\n",
      "Train Epoch: 2 [256000/1153050 (22%)] train loss: 1.346, lr: 0.00059896\n",
      "Train Epoch: 2 [307200/1153050 (27%)] train loss: 0.936, lr: 0.00059297\n",
      "Train Epoch: 2 [358400/1153050 (31%)] train loss: 1.351, lr: 0.00058704\n",
      "Train Epoch: 2 [409600/1153050 (36%)] train loss: 1.056, lr: 0.00058117\n",
      "Train Epoch: 2 [460800/1153050 (40%)] train loss: 1.366, lr: 0.00057535\n",
      "Train Epoch: 2 [512000/1153050 (44%)] train loss: 1.634, lr: 0.00056960\n",
      "Train Epoch: 2 [563200/1153050 (49%)] train loss: 0.988, lr: 0.00056391\n",
      "Train Epoch: 2 [614400/1153050 (53%)] train loss: 1.099, lr: 0.00055827\n",
      "Train Epoch: 2 [665600/1153050 (58%)] train loss: 1.363, lr: 0.00055268\n",
      "Train Epoch: 2 [716800/1153050 (62%)] train loss: 1.333, lr: 0.00054716\n",
      "Train Epoch: 2 [768000/1153050 (67%)] train loss: 1.195, lr: 0.00054169\n",
      "Train Epoch: 2 [819200/1153050 (71%)] train loss: 0.994, lr: 0.00053627\n",
      "Train Epoch: 2 [870400/1153050 (75%)] train loss: 0.956, lr: 0.00053091\n",
      "Train Epoch: 2 [921600/1153050 (80%)] train loss: 1.650, lr: 0.00052560\n",
      "Train Epoch: 2 [972800/1153050 (84%)] train loss: 1.049, lr: 0.00052034\n",
      "Train Epoch: 2 [1024000/1153050 (89%)] train loss: 1.083, lr: 0.00051514\n",
      "Train Epoch: 2 [1075200/1153050 (93%)] train loss: 1.697, lr: 0.00050999\n",
      "Train Epoch: 2 [1126400/1153050 (98%)] train loss: 1.539, lr: 0.00050489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:31<00:00, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [468416/1153050 (100%)] train loss: 1.427, val loss: 1.409, val acc: 0.650, top5: 0.862, lr: 0.00049984\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 2, val accuracy: 0.650405488732955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [0/1153050 (0%)] train loss: 1.133, lr: 0.00049984\n",
      "Train Epoch: 3 [51200/1153050 (4%)] train loss: 1.154, lr: 0.00024742\n",
      "Train Epoch: 3 [102400/1153050 (9%)] train loss: 1.301, lr: 0.00024495\n",
      "Train Epoch: 3 [153600/1153050 (13%)] train loss: 1.352, lr: 0.00024250\n",
      "Train Epoch: 3 [204800/1153050 (18%)] train loss: 1.728, lr: 0.00024007\n",
      "Train Epoch: 3 [256000/1153050 (22%)] train loss: 0.941, lr: 0.00023767\n",
      "Train Epoch: 3 [307200/1153050 (27%)] train loss: 0.995, lr: 0.00023529\n",
      "Train Epoch: 3 [358400/1153050 (31%)] train loss: 0.769, lr: 0.00023294\n",
      "Train Epoch: 3 [409600/1153050 (36%)] train loss: 1.011, lr: 0.00023061\n",
      "Train Epoch: 3 [460800/1153050 (40%)] train loss: 1.343, lr: 0.00022830\n",
      "Train Epoch: 3 [512000/1153050 (44%)] train loss: 1.100, lr: 0.00022602\n",
      "Train Epoch: 3 [563200/1153050 (49%)] train loss: 1.053, lr: 0.00022376\n",
      "Train Epoch: 3 [614400/1153050 (53%)] train loss: 1.043, lr: 0.00022152\n",
      "Train Epoch: 3 [665600/1153050 (58%)] train loss: 0.895, lr: 0.00021931\n",
      "Train Epoch: 3 [716800/1153050 (62%)] train loss: 1.010, lr: 0.00021712\n",
      "Train Epoch: 3 [768000/1153050 (67%)] train loss: 1.268, lr: 0.00021494\n",
      "Train Epoch: 3 [819200/1153050 (71%)] train loss: 1.037, lr: 0.00021280\n",
      "Train Epoch: 3 [870400/1153050 (75%)] train loss: 1.235, lr: 0.00021067\n",
      "Train Epoch: 3 [921600/1153050 (80%)] train loss: 1.313, lr: 0.00020856\n",
      "Train Epoch: 3 [972800/1153050 (84%)] train loss: 0.883, lr: 0.00020647\n",
      "Train Epoch: 3 [1024000/1153050 (89%)] train loss: 0.984, lr: 0.00020441\n",
      "Train Epoch: 3 [1075200/1153050 (93%)] train loss: 1.089, lr: 0.00020237\n",
      "Train Epoch: 3 [1126400/1153050 (98%)] train loss: 1.411, lr: 0.00020034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:32<00:00, 13.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [468416/1153050 (100%)] train loss: 0.900, val loss: 1.363, val acc: 0.659, top5: 0.869, lr: 0.00019834\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 3, val accuracy: 0.6590070014127711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/1153050 (0%)] train loss: 1.120, lr: 0.00019834\n",
      "Train Epoch: 4 [51200/1153050 (4%)] train loss: 1.041, lr: 0.00009818\n",
      "Train Epoch: 4 [102400/1153050 (9%)] train loss: 0.907, lr: 0.00009720\n",
      "Train Epoch: 4 [153600/1153050 (13%)] train loss: 0.978, lr: 0.00009622\n",
      "Train Epoch: 4 [204800/1153050 (18%)] train loss: 1.465, lr: 0.00009526\n",
      "Train Epoch: 4 [256000/1153050 (22%)] train loss: 1.347, lr: 0.00009431\n",
      "Train Epoch: 4 [307200/1153050 (27%)] train loss: 0.972, lr: 0.00009337\n",
      "Train Epoch: 4 [358400/1153050 (31%)] train loss: 1.113, lr: 0.00009243\n",
      "Train Epoch: 4 [409600/1153050 (36%)] train loss: 0.632, lr: 0.00009151\n",
      "Train Epoch: 4 [460800/1153050 (40%)] train loss: 1.206, lr: 0.00009059\n",
      "Train Epoch: 4 [512000/1153050 (44%)] train loss: 1.211, lr: 0.00008969\n",
      "Train Epoch: 4 [563200/1153050 (49%)] train loss: 0.935, lr: 0.00008879\n",
      "Train Epoch: 4 [614400/1153050 (53%)] train loss: 1.127, lr: 0.00008790\n",
      "Train Epoch: 4 [665600/1153050 (58%)] train loss: 0.927, lr: 0.00008702\n",
      "Train Epoch: 4 [716800/1153050 (62%)] train loss: 0.972, lr: 0.00008615\n",
      "Train Epoch: 4 [768000/1153050 (67%)] train loss: 1.081, lr: 0.00008529\n",
      "Train Epoch: 4 [819200/1153050 (71%)] train loss: 1.296, lr: 0.00008444\n",
      "Train Epoch: 4 [870400/1153050 (75%)] train loss: 1.188, lr: 0.00008359\n",
      "Train Epoch: 4 [921600/1153050 (80%)] train loss: 0.822, lr: 0.00008276\n",
      "Train Epoch: 4 [972800/1153050 (84%)] train loss: 1.059, lr: 0.00008193\n",
      "Train Epoch: 4 [1024000/1153050 (89%)] train loss: 1.057, lr: 0.00008111\n",
      "Train Epoch: 4 [1075200/1153050 (93%)] train loss: 0.788, lr: 0.00008030\n",
      "Train Epoch: 4 [1126400/1153050 (98%)] train loss: 1.043, lr: 0.00007950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:34<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [468416/1153050 (100%)] train loss: 1.251, val loss: 1.351, val acc: 0.661, top5: 0.871, lr: 0.00007870\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 4, val accuracy: 0.6610754232459393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [0/1153050 (0%)] train loss: 1.495, lr: 0.00007870\n",
      "Train Epoch: 5 [51200/1153050 (4%)] train loss: 1.246, lr: 0.00003896\n",
      "Train Epoch: 5 [102400/1153050 (9%)] train loss: 1.212, lr: 0.00003857\n",
      "Train Epoch: 5 [153600/1153050 (13%)] train loss: 0.858, lr: 0.00003818\n",
      "Train Epoch: 5 [204800/1153050 (18%)] train loss: 1.061, lr: 0.00003780\n",
      "Train Epoch: 5 [256000/1153050 (22%)] train loss: 1.140, lr: 0.00003742\n",
      "Train Epoch: 5 [307200/1153050 (27%)] train loss: 1.004, lr: 0.00003705\n",
      "Train Epoch: 5 [358400/1153050 (31%)] train loss: 1.095, lr: 0.00003668\n",
      "Train Epoch: 5 [409600/1153050 (36%)] train loss: 0.744, lr: 0.00003631\n",
      "Train Epoch: 5 [460800/1153050 (40%)] train loss: 1.105, lr: 0.00003595\n",
      "Train Epoch: 5 [512000/1153050 (44%)] train loss: 1.413, lr: 0.00003559\n",
      "Train Epoch: 5 [563200/1153050 (49%)] train loss: 1.081, lr: 0.00003523\n",
      "Train Epoch: 5 [614400/1153050 (53%)] train loss: 1.314, lr: 0.00003488\n",
      "Train Epoch: 5 [665600/1153050 (58%)] train loss: 0.935, lr: 0.00003453\n",
      "Train Epoch: 5 [716800/1153050 (62%)] train loss: 1.107, lr: 0.00003419\n",
      "Train Epoch: 5 [768000/1153050 (67%)] train loss: 1.405, lr: 0.00003384\n",
      "Train Epoch: 5 [819200/1153050 (71%)] train loss: 1.048, lr: 0.00003351\n",
      "Train Epoch: 5 [870400/1153050 (75%)] train loss: 1.131, lr: 0.00003317\n",
      "Train Epoch: 5 [921600/1153050 (80%)] train loss: 0.898, lr: 0.00003284\n",
      "Train Epoch: 5 [972800/1153050 (84%)] train loss: 1.200, lr: 0.00003251\n",
      "Train Epoch: 5 [1024000/1153050 (89%)] train loss: 0.905, lr: 0.00003219\n",
      "Train Epoch: 5 [1075200/1153050 (93%)] train loss: 1.207, lr: 0.00003186\n",
      "Train Epoch: 5 [1126400/1153050 (98%)] train loss: 1.116, lr: 0.00003155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:32<00:00, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [468416/1153050 (100%)] train loss: 1.303, val loss: 1.351, val acc: 0.662, top5: 0.871, lr: 0.00003123\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 5, val accuracy: 0.661707657844002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [0/1153050 (0%)] train loss: 1.329, lr: 0.00003123\n",
      "Train Epoch: 6 [51200/1153050 (4%)] train loss: 1.055, lr: 0.00001546\n",
      "Train Epoch: 6 [102400/1153050 (9%)] train loss: 1.145, lr: 0.00001530\n",
      "Train Epoch: 6 [153600/1153050 (13%)] train loss: 0.891, lr: 0.00001515\n",
      "Train Epoch: 6 [204800/1153050 (18%)] train loss: 1.306, lr: 0.00001500\n",
      "Train Epoch: 6 [256000/1153050 (22%)] train loss: 1.080, lr: 0.00001485\n",
      "Train Epoch: 6 [307200/1153050 (27%)] train loss: 1.305, lr: 0.00001470\n",
      "Train Epoch: 6 [358400/1153050 (31%)] train loss: 1.064, lr: 0.00001455\n",
      "Train Epoch: 6 [409600/1153050 (36%)] train loss: 1.093, lr: 0.00001441\n",
      "Train Epoch: 6 [460800/1153050 (40%)] train loss: 0.982, lr: 0.00001426\n",
      "Train Epoch: 6 [512000/1153050 (44%)] train loss: 0.853, lr: 0.00001412\n",
      "Train Epoch: 6 [563200/1153050 (49%)] train loss: 0.740, lr: 0.00001398\n",
      "Train Epoch: 6 [614400/1153050 (53%)] train loss: 1.277, lr: 0.00001384\n",
      "Train Epoch: 6 [665600/1153050 (58%)] train loss: 1.041, lr: 0.00001370\n",
      "Train Epoch: 6 [716800/1153050 (62%)] train loss: 0.986, lr: 0.00001357\n",
      "Train Epoch: 6 [768000/1153050 (67%)] train loss: 0.761, lr: 0.00001343\n",
      "Train Epoch: 6 [819200/1153050 (71%)] train loss: 1.091, lr: 0.00001330\n",
      "Train Epoch: 6 [870400/1153050 (75%)] train loss: 1.272, lr: 0.00001316\n",
      "Train Epoch: 6 [921600/1153050 (80%)] train loss: 0.971, lr: 0.00001303\n",
      "Train Epoch: 6 [972800/1153050 (84%)] train loss: 1.145, lr: 0.00001290\n",
      "Train Epoch: 6 [1024000/1153050 (89%)] train loss: 1.298, lr: 0.00001277\n",
      "Train Epoch: 6 [1075200/1153050 (93%)] train loss: 0.925, lr: 0.00001264\n",
      "Train Epoch: 6 [1126400/1153050 (98%)] train loss: 0.849, lr: 0.00001252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:34<00:00, 12.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [468416/1153050 (100%)] train loss: 1.245, val loss: 1.348, val acc: 0.662, top5: 0.872, lr: 0.00001239\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 6, val accuracy: 0.6622774495187992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [0/1153050 (0%)] train loss: 0.908, lr: 0.00001239\n",
      "Train Epoch: 7 [51200/1153050 (4%)] train loss: 1.399, lr: 0.00000613\n",
      "Train Epoch: 7 [102400/1153050 (9%)] train loss: 0.842, lr: 0.00000607\n",
      "Train Epoch: 7 [153600/1153050 (13%)] train loss: 0.887, lr: 0.00000601\n",
      "Train Epoch: 7 [204800/1153050 (18%)] train loss: 1.003, lr: 0.00000595\n",
      "Train Epoch: 7 [256000/1153050 (22%)] train loss: 1.183, lr: 0.00000589\n",
      "Train Epoch: 7 [307200/1153050 (27%)] train loss: 1.335, lr: 0.00000583\n",
      "Train Epoch: 7 [358400/1153050 (31%)] train loss: 0.916, lr: 0.00000578\n",
      "Train Epoch: 7 [409600/1153050 (36%)] train loss: 0.944, lr: 0.00000572\n",
      "Train Epoch: 7 [460800/1153050 (40%)] train loss: 0.817, lr: 0.00000566\n",
      "Train Epoch: 7 [512000/1153050 (44%)] train loss: 0.667, lr: 0.00000560\n",
      "Train Epoch: 7 [563200/1153050 (49%)] train loss: 0.932, lr: 0.00000555\n",
      "Train Epoch: 7 [614400/1153050 (53%)] train loss: 1.104, lr: 0.00000549\n",
      "Train Epoch: 7 [665600/1153050 (58%)] train loss: 0.926, lr: 0.00000544\n",
      "Train Epoch: 7 [716800/1153050 (62%)] train loss: 1.026, lr: 0.00000538\n",
      "Train Epoch: 7 [768000/1153050 (67%)] train loss: 0.874, lr: 0.00000533\n",
      "Train Epoch: 7 [819200/1153050 (71%)] train loss: 1.230, lr: 0.00000528\n",
      "Train Epoch: 7 [870400/1153050 (75%)] train loss: 0.817, lr: 0.00000522\n",
      "Train Epoch: 7 [921600/1153050 (80%)] train loss: 1.023, lr: 0.00000517\n",
      "Train Epoch: 7 [972800/1153050 (84%)] train loss: 0.924, lr: 0.00000512\n",
      "Train Epoch: 7 [1024000/1153050 (89%)] train loss: 1.129, lr: 0.00000507\n",
      "Train Epoch: 7 [1075200/1153050 (93%)] train loss: 1.028, lr: 0.00000502\n",
      "Train Epoch: 7 [1126400/1153050 (98%)] train loss: 1.041, lr: 0.00000497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:34<00:00, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [468416/1153050 (100%)] train loss: 0.850, val loss: 1.347, val acc: 0.663, top5: 0.872, lr: 0.00000492\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 7, val accuracy: 0.6628862680206374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [0/1153050 (0%)] train loss: 0.647, lr: 0.00000492\n",
      "Train Epoch: 8 [51200/1153050 (4%)] train loss: 0.712, lr: 0.00000243\n",
      "Train Epoch: 8 [102400/1153050 (9%)] train loss: 1.434, lr: 0.00000241\n",
      "Train Epoch: 8 [153600/1153050 (13%)] train loss: 1.191, lr: 0.00000239\n",
      "Train Epoch: 8 [204800/1153050 (18%)] train loss: 1.021, lr: 0.00000236\n",
      "Train Epoch: 8 [256000/1153050 (22%)] train loss: 0.822, lr: 0.00000234\n",
      "Train Epoch: 8 [307200/1153050 (27%)] train loss: 1.070, lr: 0.00000231\n",
      "Train Epoch: 8 [358400/1153050 (31%)] train loss: 0.972, lr: 0.00000229\n",
      "Train Epoch: 8 [409600/1153050 (36%)] train loss: 0.930, lr: 0.00000227\n",
      "Train Epoch: 8 [460800/1153050 (40%)] train loss: 1.317, lr: 0.00000225\n",
      "Train Epoch: 8 [512000/1153050 (44%)] train loss: 0.783, lr: 0.00000222\n",
      "Train Epoch: 8 [563200/1153050 (49%)] train loss: 1.002, lr: 0.00000220\n",
      "Train Epoch: 8 [614400/1153050 (53%)] train loss: 0.905, lr: 0.00000218\n",
      "Train Epoch: 8 [665600/1153050 (58%)] train loss: 1.187, lr: 0.00000216\n",
      "Train Epoch: 8 [716800/1153050 (62%)] train loss: 1.150, lr: 0.00000214\n",
      "Train Epoch: 8 [768000/1153050 (67%)] train loss: 0.745, lr: 0.00000211\n",
      "Train Epoch: 8 [819200/1153050 (71%)] train loss: 0.929, lr: 0.00000209\n",
      "Train Epoch: 8 [870400/1153050 (75%)] train loss: 1.156, lr: 0.00000207\n",
      "Train Epoch: 8 [921600/1153050 (80%)] train loss: 0.896, lr: 0.00000205\n",
      "Train Epoch: 8 [972800/1153050 (84%)] train loss: 1.129, lr: 0.00000203\n",
      "Train Epoch: 8 [1024000/1153050 (89%)] train loss: 1.036, lr: 0.00000201\n",
      "Train Epoch: 8 [1075200/1153050 (93%)] train loss: 0.954, lr: 0.00000199\n",
      "Train Epoch: 8 [1126400/1153050 (98%)] train loss: 1.085, lr: 0.00000197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:34<00:00, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [468416/1153050 (100%)] train loss: 1.359, val loss: 1.348, val acc: 0.663, top5: 0.871, lr: 0.00000195\n",
      "==================== best validation accuracy ====================\n",
      "epoch: 8, val accuracy: 0.6629331002130865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [0/1153050 (0%)] train loss: 1.316, lr: 0.00000195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m# model.classifier[0] = torch.nn.Linear(576,1280)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# model.classifier[3] = torch.nn.Linear(1280,1000)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# model = models.mobilenet_v3_large()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# model.load_state_dict(torch.load(\"best_batch_i162153.pth\")['model_state_dict'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# train_loader, val_loader,_ = load_tiny_imagenet(64,1234)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m train_loader, val_loader \u001b[39m=\u001b[39m load_imagenet(\u001b[39m64\u001b[39m,\u001b[39m12345\u001b[39m,resize\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gc28692/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train_models.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m train(model,train_loader,val_loader,\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m,lr\u001b[39m=\u001b[39;49m\u001b[39m0.004\u001b[39;49m)\n",
      "File \u001b[0;32m~/Projects/continual_learning/continual_learning_playground/src/test_time_kd/train.py:35\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, device, lr)\u001b[0m\n\u001b[1;32m     32\u001b[0m batch_iter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (data, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     36\u001b[0m         \u001b[39m# Big Forward\u001b[39;00m\n\u001b[1;32m     37\u001b[0m         data, target \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device), target\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m         optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/cl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/cl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1272\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1270\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m   1271\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_thread\u001b[39m.\u001b[39mis_alive():\n\u001b[0;32m-> 1272\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1273\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1274\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/cl/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1108\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1117\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1121\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1123\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cl/lib/python3.9/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[39mif\u001b[39;00m remaining \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait(remaining)\n\u001b[1;32m    181\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnot_full\u001b[39m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/cl/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = models.mobilenet_v3_small(weights='DEFAULT').to('cuda')\n",
    "with torch.no_grad():\n",
    "    sub_w1 = model.classifier[0].weight[:,:]\n",
    "    sub_b1 = model.classifier[0].bias[:]\n",
    "    new_layer1 = torch.nn.Linear(sub_w1.size()[1],1280)\n",
    "\n",
    "    sub_w2 = model.classifier[3].weight[:,:]\n",
    "    sub_b2 = model.classifier[3].bias[:]\n",
    "    new_layer2 = torch.nn.Linear(1280,1000)\n",
    "\n",
    "    # copy pretrained part of the weight matrix\n",
    "    new_layer1.weight[:sub_w1.size()[0],:] = sub_w1\n",
    "    new_layer1.bias[:sub_b1.size()[0]] = sub_b1\n",
    "\n",
    "    new_layer2.weight[:,:sub_w2.size()[1]] = sub_w2\n",
    "    new_layer2.bias[:sub_b2.size()[0]] = sub_b2\n",
    "\n",
    "    model.classifier[0] = new_layer1\n",
    "    model.classifier[3] = new_layer2\n",
    "    print(model.classifier)\n",
    "    # model.classifier[0] = torch.nn.Linear(576,1280)\n",
    "    # model.classifier[3] = torch.nn.Linear(1280,1000)\n",
    "# model = models.mobilenet_v3_large()\n",
    "# model.load_state_dict(torch.load(\"best_batch_i162153.pth\")['model_state_dict'])\n",
    "# train_loader, val_loader,_ = load_tiny_imagenet(64,1234)\n",
    "train_loader, val_loader = load_imagenet(64,12345,resize=128)\n",
    "train(model,train_loader,val_loader,'cuda',lr=0.004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = load_imagenet(128,1234,False)\n",
    "test_loader128 = load_imagenet(128,1234,False,resize=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV3(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
       "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
       "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
       "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (block): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "          (2): Hardswish()\n",
       "        )\n",
       "        (2): SqueezeExcitation(\n",
       "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (activation): ReLU()\n",
       "          (scale_activation): Hardsigmoid()\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (12): Conv2dNormActivation(\n",
       "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "      (2): Hardswish()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=576, out_features=1280, bias=True)\n",
       "    (1): Hardswish()\n",
       "    (2): Dropout(p=0.2, inplace=True)\n",
       "    (3): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.mobilenet_v3_large(weights='DEFAULT').to('cuda')\n",
    "model64 = models.mobilenet_v3_small()\n",
    "model64.classifier[0] = torch.nn.Linear(576,1280)\n",
    "model64.classifier[3] = torch.nn.Linear(1280,1000)\n",
    "model64.load_state_dict(torch.load(\"best_batch_i162153.pth\")['model_state_dict'])\n",
    "model64 = model64.to('cuda')\n",
    "model.eval()\n",
    "model64.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [04:11<00:00,  1.56it/s]\n"
     ]
    }
   ],
   "source": [
    "acc64_clean = validate(model64,test_loader128,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.57538, 1.8619217997621698, 0.80626)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc64_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2002/2002 [02:33<00:00, 13.06it/s]\n"
     ]
    }
   ],
   "source": [
    "acc_clean = validate(model,test_loader,'cuda')\n",
    "acc64_clean = validate(model64,test_loader128,'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41572156700515933, 2.7379780524855963, 0.6734781488795398)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc64_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobilenet V3 Large Clean (224x224)\n",
      " Top 1: 75.31%\t Top 5: 92.63%\n",
      "Mobilenet V3 Large Clean (64x64)\n",
      " Top 1: 38.34%\t Top 5: 63.53%\n"
     ]
    }
   ],
   "source": [
    "print(\"Mobilenet V3 Large Clean (224x224)\\n Top 1: {:2.2f}%\\t Top 5: {:2.2f}%\".format(acc_clean[0]*100,acc_clean[2]*100))\n",
    "print(\"Mobilenet V3 Large Clean (64x64)\\n Top 1: {:2.2f}%\\t Top 5: {:2.2f}%\".format(acc64_clean[0]*100,acc64_clean[2]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "224x224: 22.863256 5483032.0\n",
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "64x64: 22.202168 2946568.0\n"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "model = model = models.mobilenet_v3_large()\n",
    "input = torch.randn(1, 3, 224, 224)\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(\"224x224:\",macs/1e6,params)\n",
    "\n",
    "model = models.mobilenet_v3_small(weights='DEFAULT')\n",
    "model.classifier[0] = torch.nn.Linear(576,1280)\n",
    "model.classifier[3] = torch.nn.Linear(1280,1000)\n",
    "input = torch.randn(1, 3, 128, 128)\n",
    "macs, params = profile(model, inputs=(input, ))\n",
    "print(\"64x64:\",macs/1e6,params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71152ab9c07ce901c8cf95cbd74fadea2c31d5b816d92473f31e695d403a1560"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
